{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "936d5d69",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install -q langchain\n",
    "#!pip install langchain_community\n",
    "#!pip install -U sentence-transformers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aaadb15",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Read files\n",
    "with open('/content/cont_map.txt', 'r') as f:\n",
    "    all_text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3be48281",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Break down the text into smaller, manageable chunks using LangChain\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# This is the text that we are going to split\n",
    "#with open('state_of_the_union.txt') as f:\n",
    "state_of_the_union = all_text\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    # Set a really small chunk size, just to show there's a limit\n",
    "    chunk_size=100,\n",
    "    chunk_overlap=20,\n",
    "    length_function=len,\n",
    ")\n",
    "\n",
    "texts = text_splitter.create_documents([state_of_the_union])\n",
    "print(texts[0].page_content)\n",
    "texts[1].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35c65f91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vector embeddings using sentence-transformers\n",
    "\n",
    "#!pip install -U sentence-transformers\n",
    "from langchain_community.embeddings import SentenceTransformerEmbeddings\n",
    "\n",
    "# create the open-source embedding function\n",
    "embedding_function = SentenceTransformerEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "\n",
    "# embed the documents\n",
    "embeddings = embedding_function.embed_documents([text.page_content for text in texts])\n",
    "\n",
    "print(f\"Found {len(embeddings)} documents\")\n",
    "print(f\"Here's a sample of the first document: {embeddings[0][:5]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f811863a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.embeddings import SentenceTransformerEmbeddings\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "#create open source embedding functon\n",
    "embed_input = SentenceTransformerEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "\n",
    "input_keyword = input(\"Enter the keyword to search\")\n",
    "\n",
    "# 1. Embed the Query\n",
    "query_embedding = embed_input.embed_query(input_keyword)\n",
    "\n",
    "# 2. Calculate Similarity (using cosine similarity as an example)\n",
    "# Reshape the query_embedding to a 2D array for cosine_similarity\n",
    "query_embedding_2d = [query_embedding]\n",
    "\n",
    "# Calculate cosine similarity between the query embedding and all document embeddings\n",
    "similarities = cosine_similarity(query_embedding_2d, embeddings)[0]\n",
    "\n",
    "# 3. Rank and Retrieve\n",
    "# Get the indices of the top similar documents\n",
    "top_n = 5  # You can adjust the number of top results you want\n",
    "top_indices = similarities.argsort()[-top_n:][::-1]\n",
    "\n",
    "print(f\"\\nTop {top_n} most similar document chunks for the keyword '{input_keyword}':\")\n",
    "for index in top_indices:\n",
    "    print(f\"Similarity: {similarities[index]:.4f}\")\n",
    "    print(texts[index].page_content)\n",
    "    print(\"-\" * 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "144a9a49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming you have the necessary libraries for interacting with Gemini already installed\n",
    "from google.colab import auth\n",
    "#auth.authenticate_user()\n",
    "\n",
    "import google.generativeai as genai\n",
    "\n",
    "# Replace \"YOUR_API_KEY\" with your actual API key for Gemini\n",
    "genai.configure(api_key=\"Your API Key\")\n",
    "\n",
    "# Initialize the Gemini model\n",
    "#print(\"Available models:\")\n",
    "#for m in genai.list_models():\n",
    "#  if 'generateContent' in m.supported_generation_methods:\n",
    "#    print(m.name)\n",
    "\n",
    "#models/gemini-2.0-flash\n",
    "model = genai.GenerativeModel('gemini-2.0-flash')\n",
    "# Get the text from the top similar document chunks based on the previously calculated top_indices\n",
    "top_texts = [texts[index].page_content for index in top_indices]\n",
    "\n",
    "# Combine the top texts into a single context string\n",
    "context = \" \".join(top_texts)\n",
    "\n",
    "# Construct the prompt for the Gemini model\n",
    "# This prompt instructs the model to answer the question based *only* on the provided context.\n",
    "prompt = f\"\"\"Based on the following context, answer the question: '{input_keyword}'\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "If the information is not available in the context, please state that you cannot answer based on the provided information.\n",
    "\"\"\"\n",
    "\n",
    "# Send the prompt to the Gemini model and get the response\n",
    "# This is a conceptual example; the actual function call might vary depending on the Gemini library.\n",
    "response = model.generate_content(prompt)\n",
    "\n",
    "# Print the response from the Gemini model\n",
    "print(\"\\nGemini Response based on context:\")\n",
    "print(response.text)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
