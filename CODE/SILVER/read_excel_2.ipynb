{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0187117b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install langchain langchain-google-genai google-generativeai faiss-cpu python-dotenv tiktoken\n",
    "\n",
    "!pip install langchain langchain-google-genai google-generativeai faiss-cpu python-dotenv tiktoken langchain-community\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d0b1804",
   "metadata": {},
   "outputs": [],
   "source": [
    "#2-Implement a RAG system for extracting information from multiple excel sheets using LLM, Langchain, word embedding, excel sheet prompt and others tools if necessary. If possible display the extracted information in a table format\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "def merge_csv_files(input_folder):\n",
    "    # Get all CSV files in the input folder\n",
    "    csv_files = glob.glob(os.path.join(input_folder, \"*.csv\"))\n",
    "\n",
    "    # Initialize an empty list to hold DataFrames\n",
    "    dataframes = []\n",
    "\n",
    "    # Read each CSV file and append it to the list\n",
    "    for file in csv_files:\n",
    "        df = pd.read_csv(file, on_bad_lines='skip')\n",
    "        dataframes.append(df)\n",
    "\n",
    "    # Concatenate all DataFrames into a single DataFrame\n",
    "    merged_df = pd.concat(dataframes, ignore_index=True)\n",
    "\n",
    "    # Write the merged DataFrame to a new CSV file\n",
    "    #merged_df.to_csv(output_file, index=False)\n",
    "    return merged_df\n",
    "\n",
    "merged_df = merge_csv_files('/content/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ffc0b8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---  Load Environment Variables (API Key) ---\n",
    "# Assuming you have the necessary libraries for interacting with Gemini already installed\n",
    "from google.colab import auth\n",
    "#auth.authenticate_user()\n",
    "\n",
    "import google.generativeai as genai\n",
    "\n",
    "# Replace \"YOUR_API_KEY\" with your actual API key for Gemini\n",
    "genai.configure(api_key=\"API Key Here\")\n",
    "google_api_key=\"API Key Here\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb6fce84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 3. Split Documents into Chunks ---\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "print(\"Splitting documents into chunks...\")\n",
    "\n",
    "# Convert DataFrame rows to Document objects\n",
    "documents = []\n",
    "for index, row in merged_df.iterrows():\n",
    "    # Assuming you want to use all columns for the document content\n",
    "    # You might need to adjust this based on which columns contain relevant text\n",
    "    document_content = \" \".join(row.astype(str))\n",
    "    documents.append(Document(page_content=document_content))\n",
    "\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=100,\n",
    "    chunk_overlap=20,\n",
    "    length_function=len,\n",
    ")\n",
    "texts = text_splitter.split_documents(documents)\n",
    "if not texts:\n",
    "    raise ValueError(\"Document splitting resulted in no chunks.\")\n",
    "print(f\"Split into {len(texts)} chunks.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07689255",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vector embeddings using sentence-transformers\n",
    "\n",
    "!pip install -U sentence-transformers\n",
    "from langchain_community.embeddings import SentenceTransformerEmbeddings\n",
    "\n",
    "# create the open-source embedding function\n",
    "embedding_function = SentenceTransformerEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "\n",
    "# embed the documents\n",
    "embeddings = embedding_function.embed_documents([text.page_content for text in texts])\n",
    "\n",
    "print(f\"Found {len(embeddings)} documents\")\n",
    "print(f\"Here's a sample of the first document: {embeddings[0][:5]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b497949c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.embeddings import SentenceTransformerEmbeddings\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "#create open source embedding functon\n",
    "embed_input = SentenceTransformerEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "\n",
    "input_keyword = input(\"Enter the keyword to search\")\n",
    "\n",
    "# 1. Embed the Query\n",
    "query_embedding = embed_input.embed_query(input_keyword)\n",
    "\n",
    "# 2. Calculate Similarity (using cosine similarity as an example)\n",
    "# Reshape the query_embedding to a 2D array for cosine_similarity\n",
    "query_embedding_2d = [query_embedding]\n",
    "\n",
    "# Calculate cosine similarity between the query embedding and all document embeddings\n",
    "similarities = cosine_similarity(query_embedding_2d, embeddings)[0]\n",
    "\n",
    "# 3. Rank and Retrieve\n",
    "# Get the indices of the top similar documents\n",
    "top_n = 5  # You can adjust the number of top results you want\n",
    "top_indices = similarities.argsort()[-top_n:][::-1]\n",
    "\n",
    "print(f\"\\nTop {top_n} most similar document chunks for the keyword '{input_keyword}':\")\n",
    "for index in top_indices:\n",
    "    print(f\"Similarity: {similarities[index]:.4f}\")\n",
    "    print(texts[index].page_content)\n",
    "    print(\"-\" * 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3cc3654",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming you have the necessary libraries for interacting with Gemini already installed\n",
    "from google.colab import auth\n",
    "#auth.authenticate_user()\n",
    "\n",
    "import google.generativeai as genai\n",
    "\n",
    "# Replace \"YOUR_API_KEY\" with your actual API key for Gemini\n",
    "genai.configure(api_key=\"API Key Here\")\n",
    "\n",
    "# Initialize the Gemini model\n",
    "#print(\"Available models:\")\n",
    "#for m in genai.list_models():\n",
    "#  if 'generateContent' in m.supported_generation_methods:\n",
    "#    print(m.name)\n",
    "\n",
    "#models/gemini-2.0-flash\n",
    "model = genai.GenerativeModel('gemini-2.0-flash')\n",
    "# Get the text from the top similar document chunks based on the previously calculated top_indices\n",
    "top_texts = [texts[index].page_content for index in top_indices]\n",
    "\n",
    "# Combine the top texts into a single context string\n",
    "context = \" \".join(top_texts)\n",
    "\n",
    "# Construct the prompt for the Gemini model\n",
    "# This prompt instructs the model to answer the question based *only* on the provided context.\n",
    "prompt = f\"\"\"Based on the following context, answer the question: '{input_keyword}'\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "If the information is not available in the context, please state that you cannot answer based on the provided information.\n",
    "\"\"\"\n",
    "\n",
    "# Send the prompt to the Gemini model and get the response\n",
    "# This is a conceptual example; the actual function call might vary depending on the Gemini library.\n",
    "response = model.generate_content(prompt)\n",
    "\n",
    "# Print the response from the Gemini model\n",
    "print(\"\\nGemini Response based on context:\")\n",
    "print(response.text)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
