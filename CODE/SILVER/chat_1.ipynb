{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "945afc61",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install langchain langchain-google-genai google-generativeai faiss-cpu python-dotenv tiktoken\n",
    "\n",
    "!pip install langchain langchain-google-genai google-generativeai faiss-cpu python-dotenv tiktoken langchain-community"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f35752d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "#from dotenv import load_dotenv\n",
    "\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings, ChatGoogleGenerativeAI\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "import google.generativeai as genai # For configuring the API key globally\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50a0d401",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1. Load Environment Variables (API Key) ---\n",
    "# Assuming you have the necessary libraries for interacting with Gemini already installed\n",
    "from google.colab import auth\n",
    "#auth.authenticate_user()\n",
    "\n",
    "import google.generativeai as genai\n",
    "\n",
    "# Replace \"YOUR_API_KEY\" with your actual API key for Gemini\n",
    "genai.configure(api_key=\"API Key\")\n",
    "google_api_key=\"API Key\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65c6e750",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 2. Load Knowledge Base ---\n",
    "print(\"Loading documents...\")\n",
    "# loader = TextLoader(\"/content/\") # This was trying to load a directory\n",
    "loader = TextLoader(\"/content/cont_map.txt\") # Provide the path to your text file\n",
    "documents = loader.load()\n",
    "if not documents:\n",
    "    raise ValueError(\"No documents loaded. Check the file path and content.\")\n",
    "print(f\"Loaded {len(documents)} document(s).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5748823e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 3. Split Documents into Chunks ---\n",
    "print(\"Splitting documents into chunks...\")\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=100,\n",
    "    chunk_overlap=20,\n",
    "    length_function=len,\n",
    ")\n",
    "chunks = text_splitter.split_documents(documents)\n",
    "if not chunks:\n",
    "    raise ValueError(\"Document splitting resulted in no chunks.\")\n",
    "print(f\"Split into {len(chunks)} chunks.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9d31389",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 4. Create Embeddings and Vector Store ---\n",
    "print(\"Creating embeddings and vector store...\")\n",
    "# Using Google's embedding model\n",
    "# Make sure to use a model that supports embedding, e.g., \"models/embedding-001\"\n",
    "# List available models:\n",
    "# for m in genai.list_models():\n",
    "#   if 'embedContent' in m.supported_generation_methods:\n",
    "#     print(m.name)\n",
    "embeddings_model_name = \"models/embedding-001\"\n",
    "embeddings_model = GoogleGenerativeAIEmbeddings(model=embeddings_model_name, google_api_key=google_api_key)\n",
    "\n",
    "# Using FAISS as the vector store\n",
    "try:\n",
    "    vectorstore = FAISS.from_documents(documents=chunks, embedding=embeddings_model)\n",
    "    print(\"Vector store created successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error creating vector store: {e}\")\n",
    "    print(\"This might be due to API restrictions or an issue with the embedding model.\")\n",
    "    print(\"Ensure your Google AI API key is valid and has the Generative Language API enabled.\")\n",
    "    exit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff8b642f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 5. Create Retriever ---\n",
    "retriever = vectorstore.as_retriever(\n",
    "    search_type=\"similarity\",\n",
    "    search_kwargs={'k': 3}\n",
    ")\n",
    "print(\"Retriever created.\")\n",
    "\n",
    "# --- 6. Define the LLM (Gemini) and Prompt Template ---\n",
    "print(\"Defining LLM and prompt template...\")\n",
    "# Using Gemini Pro model\n",
    "llm = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-2.0-flash\",\n",
    "    google_api_key=google_api_key,\n",
    "    temperature=0.7,\n",
    "    # Gemini models can be sensitive to system prompts.\n",
    "    # Sometimes, converting the system message to a human message works better or is required.\n",
    "    # However, recent LangChain versions handle this better.\n",
    "    # convert_system_message_to_human=True # Uncomment if you face issues with system prompts\n",
    ")\n",
    "\n",
    "prompt_template = \"\"\"\n",
    "You are an assistant for question-answering tasks.\n",
    "Use the following pieces of retrieved context to answer the question.\n",
    "If you don't know the answer based on the context, just say that you don't know.\n",
    "Do not try to make up an answer.\n",
    "Use three sentences maximum and keep the answer concise.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question:\n",
    "{question}\n",
    "\n",
    "Answer:\n",
    "\"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(prompt_template)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c581a248",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 7. Create the RAG Chain ---\n",
    "print(\"Creating RAG chain...\")\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "rag_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "print(\"RAG chain created.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "496d3bf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 8. Ask Questions! ---\n",
    "def ask_question(query):\n",
    "    print(f\"\\n❓ Question: {query}\")\n",
    "    try:\n",
    "        answer = rag_chain.invoke(query)\n",
    "        print(f\"✅ Answer: {answer}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error during RAG chain invocation: {e}\")\n",
    "        if \"blocked\" in str(e).lower() or \"safety\" in str(e).lower():\n",
    "            print(\"The response might have been blocked due to safety settings. \")\n",
    "            print(\"You can try adjusting safety settings in the ChatGoogleGenerativeAI constructor if needed for your use case,\")\n",
    "            print(\"or rephrasing the query/context if it triggered a safety filter.\")\n",
    "\n",
    "\n",
    "print(\"\\n--- Starting Q&A ---\")\n",
    "ask_question(\"Explain memory mapping?\")\n",
    "ask_question(\"What is available DATA memory?\")\n",
    "ask_question(\"How many segment\")\n",
    "ask_question(\"What is induction?\") # This should result in \"I don't know\"\n",
    "\n",
    "print(\"\\n--- Q&A Finished ---\")\n",
    "\n",
    "# To allow for interactive questioning:\n",
    "# print(\"\\nEnter your questions (type 'exit' to quit):\")\n",
    "# while True:\n",
    "#     user_query = input(\"> \")\n",
    "#     if user_query.lower() == 'exit':\n",
    "#         break\n",
    "#     if user_query.strip():\n",
    "#         ask_question(user_query)\n",
    "#     else:\n",
    "#         print(\"Please enter a question.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cec70a2",
   "metadata": {},
   "source": [
    "--- Starting Q&A ---\n",
    "\n",
    " Question: Explain memory mapping?\n",
    "\n",
    " Answer: I am sorry, but the context provided does not have information about memory mapping.\n",
    "\n",
    " Question: What is available DATA memory?\n",
    "\n",
    " Answer: Available DATA memory is 41'548 bytes, with an additional 164 absolute bytes.\n",
    "\n",
    " Question: How many segment\n",
    "\n",
    " Answer: The context mentions segment part 152, segment part 161, segment part 146, segment part 151, segment part 143, and segment part 144. Therefore, the context mentions six segments.\n",
    "\n",
    " Question: What is induction?\n",
    "\n",
    " Answer: I am sorry, but the provided context does not contain information about induction. Therefore, I cannot answer the question.\n",
    "\n",
    "--- Q&A Finished ---"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
