{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb880e63",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install pypdf\n",
    "#!pip install -q langchain\n",
    "#!pip install langchain_community\n",
    "#!pip install -U sentence-transformers\n",
    "print(\"\"\"\n",
    "This script performs the following steps:\n",
    "1. Reads text from a PDF file named 'LCD controller ST7066U.pdf'.\n",
    "2. Splits the extracted text into smaller chunks using `RecursiveCharacterTextSplitter`.\n",
    "3. Uses the `SentenceTransformerEmbeddings` model 'all-MiniLM-L6-v2' to create embeddings for each text chunk.\n",
    "4. Prompts the user to enter a search keyword.\n",
    "5. Creates an embedding for the user's keyword.\n",
    "6. Calculates the cosine similarity between the keyword embedding and all the document chunk embeddings.\n",
    "7. Identifies and prints the top 3 most similar document chunks along with their similarity scores.\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "052b679c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Read pdf as test\n",
    "from pypdf import PdfReader\n",
    "reader = PdfReader('/content/LCD controller ST7066U.pdf')\n",
    "number_of_pages = len(reader.pages)\n",
    "all_text = \"\"  # Initialize an empty string to store text from all pages\n",
    "\n",
    "for page_num in range(number_of_pages):\n",
    "  page = reader.pages[page_num]\n",
    "  text = page.extract_text()\n",
    "  all_text += text  # Append text from the current page to the all_text string\n",
    "\n",
    "print(\"Text from all pages:\")\n",
    "print(all_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2b4488c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Break down the text into smaller, manageable chunks using LangChain\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# This is the text that we are going to split\n",
    "#with open('state_of_the_union.txt') as f:\n",
    "state_of_the_union = all_text\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    # Set a really small chunk size, just to show there's a limit\n",
    "    chunk_size=100,\n",
    "    chunk_overlap=20,\n",
    "    length_function=len,\n",
    ")\n",
    "\n",
    "texts = text_splitter.create_documents([state_of_the_union])\n",
    "print(texts[0].page_content)\n",
    "texts[1].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3c46ac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vector embeddings using sentence-transformers\n",
    "\n",
    "#!pip install -U sentence-transformers\n",
    "from langchain_community.embeddings import SentenceTransformerEmbeddings\n",
    "\n",
    "# create the open-source embedding function\n",
    "embedding_function = SentenceTransformerEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "\n",
    "# embed the documents\n",
    "embeddings = embedding_function.embed_documents([text.page_content for text in texts])\n",
    "\n",
    "print(f\"Found {len(embeddings)} documents\")\n",
    "print(f\"Here's a sample of the first document: {embeddings[0][:5]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ef033b7",
   "metadata": {},
   "source": [
    "Popular Models (available via sentence-transformers):\n",
    "*   `all-MiniLM-L6-v2`: Very fast, good quality, small model size. Excellent general-purpose starting point.\n",
    "*   `all-mpnet-base-v2`: Higher quality than MiniLM, slightly larger and slower. Often a top performer on leaderboards.\n",
    "*   `multi-qa-mpnet-base-dot-v1`: Excellent for semantic search/question answering retrieval tasks (use dot-product similarity).\n",
    "*   `e5-large-v2` (or other `e5` variants like `multilingual-e5-large`): Often state-of-the-art open-source models.\n",
    "*   `bge-large-en-v1.5` (or other BGE variants): Strong open-source models from "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a7ccaf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.embeddings import SentenceTransformerEmbeddings\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "#create open source embedding functon\n",
    "embed_input = SentenceTransformerEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "\n",
    "input_keyword = input(\"Enter the keyword to search\")\n",
    "\n",
    "# 1. Embed the Query\n",
    "query_embedding = embed_input.embed_query(input_keyword)\n",
    "\n",
    "# 2. Calculate Similarity (using cosine similarity as an example)\n",
    "# Reshape the query_embedding to a 2D array for cosine_similarity\n",
    "query_embedding_2d = [query_embedding]\n",
    "\n",
    "# Calculate cosine similarity between the query embedding and all document embeddings\n",
    "similarities = cosine_similarity(query_embedding_2d, embeddings)[0]\n",
    "\n",
    "# 3. Rank and Retrieve\n",
    "# Get the indices of the top similar documents\n",
    "top_n = 3  # You can adjust the number of top results you want\n",
    "top_indices = similarities.argsort()[-top_n:][::-1]\n",
    "\n",
    "print(f\"\\nTop {top_n} most similar document chunks for the keyword '{input_keyword}':\")\n",
    "for index in top_indices:\n",
    "    print(f\"Similarity: {similarities[index]:.4f}\")\n",
    "    print(texts[index].page_content)\n",
    "    print(\"-\" * 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ecb2279",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming you have the necessary libraries for interacting with Gemini already installed\n",
    "from google.colab import auth\n",
    "#auth.authenticate_user()\n",
    "\n",
    "import google.generativeai as genai\n",
    "\n",
    "# Replace \"YOUR_API_KEY\" with your actual API key for Gemini\n",
    "genai.configure(api_key=\"your API Key\")\n",
    "\n",
    "# Initialize the Gemini model\n",
    "#print(\"Available models:\")\n",
    "#for m in genai.list_models():\n",
    "#  if 'generateContent' in m.supported_generation_methods:\n",
    "#    print(m.name)\n",
    "\n",
    "#models/gemini-2.0-flash\n",
    "model = genai.GenerativeModel('gemini-2.0-flash')\n",
    "# Get the text from the top similar document chunks based on the previously calculated top_indices\n",
    "top_texts = [texts[index].page_content for index in top_indices]\n",
    "\n",
    "# Combine the top texts into a single context string\n",
    "context = \" \".join(top_texts)\n",
    "\n",
    "# Construct the prompt for the Gemini model\n",
    "# This prompt instructs the model to answer the question based *only* on the provided context.\n",
    "prompt = f\"\"\"Based on the following context, answer the question: '{input_keyword}'\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "If the information is not available in the context, please state that you cannot answer based on the provided information.\n",
    "\"\"\"\n",
    "\n",
    "# Send the prompt to the Gemini model and get the response\n",
    "# This is a conceptual example; the actual function call might vary depending on the Gemini library.\n",
    "response = model.generate_content(prompt)\n",
    "\n",
    "# Print the response from the Gemini model\n",
    "print(\"\\nGemini Response based on context:\")\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da9b58b5",
   "metadata": {},
   "source": [
    "Gemini Response based on context:\n",
    "Based on the provided context, you can control the cursor in the following ways:\n",
    "\n",
    "*   **Cursor ON/OFF:** Use the \"C\" bit (8th line). Setting C to \"High\" turns the cursor on.\n",
    "*   **Moving direction:** You can set the moving direction of the cursor (and display) using the \"Entry Mode Set\".\n",
    "*   **Cursor shift:** You can shift the cursor.\n",
    "\n",
    "The context also mentions that you can bring the cursor to the left edge without changing anything else. However, it doesn't specify *how* to do this, only that it's possible. The context also references other potential controls, such as cursor blinking, but doesn't explain how to achieve them.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
