{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb880e63",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install pypdf\n",
    "#!pip install -q langchain\n",
    "#!pip install langchain_community\n",
    "#!pip install -U sentence-transformers\n",
    "print(\"\"\"\n",
    "This script performs the following steps:\n",
    "1. Reads text from a PDF file named 'LCD controller ST7066U.pdf'.\n",
    "2. Splits the extracted text into smaller chunks using `RecursiveCharacterTextSplitter`.\n",
    "3. Uses the `SentenceTransformerEmbeddings` model 'all-MiniLM-L6-v2' to create embeddings for each text chunk.\n",
    "4. Prompts the user to enter a search keyword.\n",
    "5. Creates an embedding for the user's keyword.\n",
    "6. Calculates the cosine similarity between the keyword embedding and all the document chunk embeddings.\n",
    "7. Identifies and prints the top 3 most similar document chunks along with their similarity scores.\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "052b679c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Read pdf as test\n",
    "from pypdf import PdfReader\n",
    "reader = PdfReader('/content/LCD controller ST7066U.pdf')\n",
    "number_of_pages = len(reader.pages)\n",
    "all_text = \"\"  # Initialize an empty string to store text from all pages\n",
    "\n",
    "for page_num in range(number_of_pages):\n",
    "  page = reader.pages[page_num]\n",
    "  text = page.extract_text()\n",
    "  all_text += text  # Append text from the current page to the all_text string\n",
    "\n",
    "print(\"Text from all pages:\")\n",
    "print(all_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2b4488c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt: Break down the text into smaller, manageable chunks using LangChain\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# This is the text that we are going to split\n",
    "#with open('state_of_the_union.txt') as f:\n",
    "state_of_the_union = all_text\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    # Set a really small chunk size, just to show there's a limit\n",
    "    chunk_size=100,\n",
    "    chunk_overlap=20,\n",
    "    length_function=len,\n",
    ")\n",
    "\n",
    "texts = text_splitter.create_documents([state_of_the_union])\n",
    "print(texts[0].page_content)\n",
    "texts[1].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3c46ac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt: vector embeddings using sentence-transformers\n",
    "\n",
    "#!pip install -U sentence-transformers\n",
    "from langchain_community.embeddings import SentenceTransformerEmbeddings\n",
    "\n",
    "# create the open-source embedding function\n",
    "embedding_function = SentenceTransformerEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "\n",
    "# embed the documents\n",
    "embeddings = embedding_function.embed_documents([text.page_content for text in texts])\n",
    "\n",
    "print(f\"Found {len(embeddings)} documents\")\n",
    "print(f\"Here's a sample of the first document: {embeddings[0][:5]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ef033b7",
   "metadata": {},
   "source": [
    "Popular Models (available via sentence-transformers):\n",
    "*   `all-MiniLM-L6-v2`: Very fast, good quality, small model size. Excellent general-purpose starting point.\n",
    "*   `all-mpnet-base-v2`: Higher quality than MiniLM, slightly larger and slower. Often a top performer on leaderboards.\n",
    "*   `multi-qa-mpnet-base-dot-v1`: Excellent for semantic search/question answering retrieval tasks (use dot-product similarity).\n",
    "*   `e5-large-v2` (or other `e5` variants like `multilingual-e5-large`): Often state-of-the-art open-source models.\n",
    "*   `bge-large-en-v1.5` (or other BGE variants): Strong open-source models from "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a7ccaf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.embeddings import SentenceTransformerEmbeddings\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "#create open source embedding functon\n",
    "embed_input = SentenceTransformerEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "\n",
    "input_keyword = input(\"Enter the keyword to search\")\n",
    "\n",
    "# 1. Embed the Query\n",
    "query_embedding = embed_input.embed_query(input_keyword)\n",
    "\n",
    "# 2. Calculate Similarity (using cosine similarity as an example)\n",
    "# Reshape the query_embedding to a 2D array for cosine_similarity\n",
    "query_embedding_2d = [query_embedding]\n",
    "\n",
    "# Calculate cosine similarity between the query embedding and all document embeddings\n",
    "similarities = cosine_similarity(query_embedding_2d, embeddings)[0]\n",
    "\n",
    "# 3. Rank and Retrieve\n",
    "# Get the indices of the top similar documents\n",
    "top_n = 3  # You can adjust the number of top results you want\n",
    "top_indices = similarities.argsort()[-top_n:][::-1]\n",
    "\n",
    "print(f\"\\nTop {top_n} most similar document chunks for the keyword '{input_keyword}':\")\n",
    "for index in top_indices:\n",
    "    print(f\"Similarity: {similarities[index]:.4f}\")\n",
    "    print(texts[index].page_content)\n",
    "    print(\"-\" * 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ecb2279",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the text from the top 3 most similar document chunks\n",
    "top_texts = [texts[index].page_content for index in top_indices]\n",
    "\n",
    "# You can now use these 'top_texts' as context for an LLM to generate a response.\n",
    "# This specific code snippet doesn't include the LLM generation part,\n",
    "# but the 'top_texts' variable holds the relevant context based on the search keyword.\n",
    "\n",
    "# Example of how you might pass this to an LLM (this is conceptual, not executable without an LLM library)\n",
    "# llm_response = llm.generate(prompt=f\"Based on the following context, answer the question: '{input_keyword}'\\n\\nContext:\\n{' '.join(top_texts)}\")\n",
    "# print(\"\\nLLM Response based on context:\")\n",
    "# print(llm_response)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
